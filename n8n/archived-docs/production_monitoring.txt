# Production Monitoring Implementation for Haystack Tree Navigation

## Issue Description

The current Haystack tree navigation implementation lacks comprehensive monitoring and observability features required for production deployment. This issue outlines the implementation of a complete monitoring solution to track performance, errors, usage patterns, and system health.

## Problem Statement

Without proper monitoring, we cannot:
- Identify performance bottlenecks in tree navigation operations
- Track error rates and failure patterns
- Understand usage patterns and optimize accordingly
- Alert on system degradation before users are impacted
- Debug production issues effectively
- Capacity plan based on actual usage metrics

## Proposed Solution

Implement a multi-layered monitoring approach using industry-standard tools and practices:

### 1. Metrics Collection Layer

#### Implementation: Custom Metrics Collector

```python
# File: n8n/haystack-service/monitoring/metrics_collector.py

from typing import Dict, List, Optional
from datetime import datetime
from collections import defaultdict, deque
import time
import asyncio
from enum import Enum
from prometheus_client import Counter, Histogram, Gauge, generate_latest
import structlog

class MetricType(Enum):
    COUNTER = "counter"
    HISTOGRAM = "histogram"
    GAUGE = "gauge"

class HaystackMetricsCollector:
    """Centralized metrics collection for Haystack operations"""
    
    def __init__(self):
        # Operation counters
        self.operation_total = Counter(
            'haystack_operations_total',
            'Total number of operations',
            ['operation', 'status', 'workflow_id']
        )
        
        # Operation duration histograms
        self.operation_duration = Histogram(
            'haystack_operation_duration_seconds',
            'Operation duration in seconds',
            ['operation'],
            buckets=(0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0)
        )
        
        # Document metrics
        self.documents_processed = Counter(
            'haystack_documents_processed_total',
            'Total documents processed',
            ['operation', 'document_type']
        )
        
        # Tree metrics
        self.tree_depth = Histogram(
            'haystack_tree_depth',
            'Tree depth distribution',
            buckets=(1, 2, 3, 5, 10, 20)
        )
        
        self.tree_size = Histogram(
            'haystack_tree_size_nodes',
            'Number of nodes in tree',
            buckets=(1, 10, 50, 100, 500, 1000, 5000)
        )
        
        # Error metrics
        self.errors_total = Counter(
            'haystack_errors_total',
            'Total number of errors',
            ['operation', 'error_type']
        )
        
        # System metrics
        self.active_workflows = Gauge(
            'haystack_active_workflows',
            'Number of active workflows'
        )
        
        self.elasticsearch_health = Gauge(
            'haystack_elasticsearch_health',
            'Elasticsearch cluster health (0=red, 1=yellow, 2=green)'
        )
        
        # Performance metrics
        self.response_size_bytes = Histogram(
            'haystack_response_size_bytes',
            'Response size in bytes',
            ['operation'],
            buckets=(1024, 10240, 102400, 1048576, 10485760)  # 1KB, 10KB, 100KB, 1MB, 10MB
        )
        
        # Initialize structured logger
        self.logger = structlog.get_logger(__name__)
        
    async def record_operation(
        self,
        operation: str,
        duration: float,
        success: bool,
        workflow_id: Optional[str] = None,
        metadata: Optional[Dict] = None
    ):
        """Record metrics for an operation"""
        status = "success" if success else "error"
        
        # Update counters
        self.operation_total.labels(
            operation=operation,
            status=status,
            workflow_id=workflow_id or "unknown"
        ).inc()
        
        # Record duration
        self.operation_duration.labels(operation=operation).observe(duration)
        
        # Log structured data
        self.logger.info(
            "operation_completed",
            operation=operation,
            duration=duration,
            success=success,
            workflow_id=workflow_id,
            metadata=metadata
        )
    
    async def record_tree_metrics(self, tree_data: Dict):
        """Record tree-specific metrics"""
        if 'depth' in tree_data:
            self.tree_depth.observe(tree_data['depth'])
        
        if 'node_count' in tree_data:
            self.tree_size.observe(tree_data['node_count'])
    
    async def record_error(
        self,
        operation: str,
        error_type: str,
        error_details: str,
        workflow_id: Optional[str] = None
    ):
        """Record error metrics"""
        self.errors_total.labels(
            operation=operation,
            error_type=error_type
        ).inc()
        
        self.logger.error(
            "operation_error",
            operation=operation,
            error_type=error_type,
            error_details=error_details,
            workflow_id=workflow_id
        )

# Global metrics instance
metrics = HaystackMetricsCollector()
```

#### Integration: Endpoint Decorators

```python
# File: n8n/haystack-service/monitoring/decorators.py

import time
import functools
import asyncio
from typing import Callable, Any
import sys

def monitor_endpoint(operation_name: str):
    """Decorator to monitor FastAPI endpoints"""
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        async def async_wrapper(*args, **kwargs):
            start_time = time.time()
            workflow_id = None
            success = False
            error_type = None
            
            try:
                # Extract workflow_id from arguments if available
                if 'workflow_id' in kwargs:
                    workflow_id = kwargs['workflow_id']
                elif len(args) > 0 and hasattr(args[0], 'workflow_id'):
                    workflow_id = args[0].workflow_id
                
                # Execute the function
                result = await func(*args, **kwargs)
                success = True
                
                # Extract metrics from result if available
                if hasattr(result, 'dict'):
                    result_dict = result.dict()
                    if 'tree_metadata' in result_dict:
                        await metrics.record_tree_metrics(result_dict['tree_metadata'])
                
                return result
                
            except Exception as e:
                error_type = type(e).__name__
                await metrics.record_error(
                    operation=operation_name,
                    error_type=error_type,
                    error_details=str(e),
                    workflow_id=workflow_id
                )
                raise
                
            finally:
                duration = time.time() - start_time
                await metrics.record_operation(
                    operation=operation_name,
                    duration=duration,
                    success=success,
                    workflow_id=workflow_id,
                    metadata={
                        "error_type": error_type,
                        "response_size": sys.getsizeof(result) if success else 0
                    }
                )
        
        @functools.wraps(func)
        def sync_wrapper(*args, **kwargs):
            # Handle synchronous functions
            return asyncio.run(async_wrapper(*args, **kwargs))
        
        return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper
    
    return decorator
```

### 2. Application Integration

#### Update Endpoints with Monitoring

```python
# File: n8n/haystack-service/haystack_service_simple.py (modifications)

from monitoring.decorators import monitor_endpoint
from monitoring.metrics_collector import metrics

# Add monitoring to endpoints
@app.get("/get_final_summary/{workflow_id}", response_model=FinalSummaryResponse)
@monitor_endpoint("get_final_summary")
async def get_final_summary(workflow_id: str):
    # Existing implementation...
    pass

@app.get("/get_complete_tree/{workflow_id}", response_model=CompleteTreeResponse)
@monitor_endpoint("get_complete_tree")
async def get_complete_tree(workflow_id: str, max_depth: int = 5, include_content: bool = False):
    # Existing implementation...
    pass

@app.get("/get_document_with_context/{document_id}", response_model=DocumentContentResponse)
@monitor_endpoint("get_document_with_context")
async def get_document_with_context(
    document_id: str,
    include_full_content: bool = True,
    include_siblings: bool = False
):
    # Existing implementation...
    pass

# Add metrics endpoint
@app.get("/metrics")
async def get_metrics():
    """Prometheus metrics endpoint"""
    return Response(generate_latest(), media_type="text/plain")

# Add health check with detailed status
@app.get("/health/detailed")
async def detailed_health_check():
    """Detailed health check with subsystem status"""
    health_status = {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "subsystems": {}
    }
    
    # Check Elasticsearch
    try:
        es_health = es_client.cluster.health()
        health_status["subsystems"]["elasticsearch"] = {
            "status": es_health["status"],
            "number_of_nodes": es_health["number_of_nodes"],
            "active_shards": es_health["active_shards"]
        }
        
        # Update Prometheus metric
        health_value = {"green": 2, "yellow": 1, "red": 0}.get(es_health["status"], 0)
        metrics.elasticsearch_health.set(health_value)
        
    except Exception as e:
        health_status["subsystems"]["elasticsearch"] = {
            "status": "error",
            "error": str(e)
        }
        health_status["status"] = "unhealthy"
    
    # Check embedding model
    try:
        if embedding_model:
            test_embedding = embedding_model.encode("test")
            health_status["subsystems"]["embedding_model"] = {
                "status": "healthy",
                "model_name": EMBEDDING_MODEL
            }
        else:
            health_status["subsystems"]["embedding_model"] = {
                "status": "not_initialized"
            }
    except Exception as e:
        health_status["subsystems"]["embedding_model"] = {
            "status": "error",
            "error": str(e)
        }
        health_status["status"] = "degraded"
    
    # Calculate active workflows
    try:
        workflow_count_query = {
            "aggs": {
                "unique_workflows": {
                    "cardinality": {
                        "field": "workflow.workflow_id.keyword"
                    }
                }
            },
            "size": 0
        }
        result = es_client.search(index=ELASTICSEARCH_INDEX, body=workflow_count_query)
        active_workflows = result["aggregations"]["unique_workflows"]["value"]
        metrics.active_workflows.set(active_workflows)
        
        health_status["subsystems"]["workflows"] = {
            "active_count": active_workflows
        }
    except Exception as e:
        health_status["subsystems"]["workflows"] = {
            "status": "error",
            "error": str(e)
        }
    
    return health_status
```

### 3. Distributed Tracing

#### Implementation: OpenTelemetry Integration

```python
# File: n8n/haystack-service/monitoring/tracing.py

from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.instrumentation.elasticsearch import ElasticsearchInstrumentor
from opentelemetry.instrumentation.httpx import HTTPXClientInstrumentor
import os

def setup_tracing(app):
    """Configure OpenTelemetry tracing"""
    
    # Set up the tracer provider
    trace.set_tracer_provider(TracerProvider())
    tracer = trace.get_tracer(__name__)
    
    # Configure OTLP exporter (for Jaeger/Tempo)
    otlp_exporter = OTLPSpanExporter(
        endpoint=os.getenv("OTEL_EXPORTER_OTLP_ENDPOINT", "localhost:4317"),
        insecure=True
    )
    
    # Add span processor
    span_processor = BatchSpanProcessor(otlp_exporter)
    trace.get_tracer_provider().add_span_processor(span_processor)
    
    # Instrument FastAPI
    FastAPIInstrumentor.instrument_app(app)
    
    # Instrument Elasticsearch
    ElasticsearchInstrumentor().instrument()
    
    # Instrument HTTP client
    HTTPXClientInstrumentor().instrument()
    
    return tracer

# Custom span decorator for tree operations
def trace_operation(operation_name: str):
    """Decorator to add custom tracing spans"""
    def decorator(func):
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            tracer = trace.get_tracer(__name__)
            
            with tracer.start_as_current_span(operation_name) as span:
                # Add span attributes
                if 'workflow_id' in kwargs:
                    span.set_attribute("workflow.id", kwargs['workflow_id'])
                
                if 'max_depth' in kwargs:
                    span.set_attribute("tree.max_depth", kwargs['max_depth'])
                
                try:
                    result = await func(*args, **kwargs)
                    span.set_status(trace.Status(trace.StatusCode.OK))
                    return result
                except Exception as e:
                    span.set_status(
                        trace.Status(trace.StatusCode.ERROR, str(e))
                    )
                    span.record_exception(e)
                    raise
        
        return wrapper
    return decorator
```

### 4. Logging Infrastructure

#### Structured Logging Configuration

```python
# File: n8n/haystack-service/monitoring/logging_config.py

import structlog
import logging
import sys
from pythonjsonlogger import jsonlogger

def setup_structured_logging():
    """Configure structured JSON logging"""
    
    # Configure structlog
    structlog.configure(
        processors=[
            structlog.stdlib.filter_by_level,
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            structlog.processors.CallsiteParameterAdder(
                parameters=[
                    structlog.processors.CallsiteParameter.FILENAME,
                    structlog.processors.CallsiteParameter.FUNC_NAME,
                    structlog.processors.CallsiteParameter.LINENO,
                ]
            ),
            structlog.processors.dict_tracebacks,
            structlog.processors.JSONRenderer()
        ],
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        cache_logger_on_first_use=True,
    )
    
    # Configure Python's logging
    handler = logging.StreamHandler(sys.stdout)
    handler.setFormatter(jsonlogger.JsonFormatter(
        fmt='%(timestamp)s %(level)s %(name)s %(message)s',
        datefmt='%Y-%m-%dT%H:%M:%S'
    ))
    
    logging.basicConfig(
        handlers=[handler],
        level=logging.INFO
    )
    
    # Suppress noisy loggers
    logging.getLogger("elasticsearch").setLevel(logging.WARNING)
    logging.getLogger("urllib3").setLevel(logging.WARNING)

# Logging middleware for request/response
class LoggingMiddleware:
    def __init__(self, app):
        self.app = app
        self.logger = structlog.get_logger(__name__)
    
    async def __call__(self, scope, receive, send):
        if scope["type"] == "http":
            request_id = str(uuid.uuid4())
            
            # Log request
            self.logger.info(
                "http_request_started",
                request_id=request_id,
                method=scope["method"],
                path=scope["path"],
                query_string=scope.get("query_string", b"").decode(),
                client=scope.get("client")
            )
            
            start_time = time.time()
            
            async def send_wrapper(message):
                if message["type"] == "http.response.start":
                    duration = time.time() - start_time
                    
                    # Log response
                    self.logger.info(
                        "http_request_completed",
                        request_id=request_id,
                        status_code=message["status"],
                        duration=duration
                    )
                
                await send(message)
            
            await self.app(scope, receive, send_wrapper)
        else:
            await self.app(scope, receive, send)
```

### 5. Alerting Rules

#### Prometheus Alert Configuration

```yaml
# File: n8n/haystack-service/monitoring/alerts.yml

groups:
  - name: haystack_alerts
    interval: 30s
    rules:
      # High error rate alert
      - alert: HaystackHighErrorRate
        expr: |
          (
            sum(rate(haystack_errors_total[5m])) /
            sum(rate(haystack_operations_total[5m]))
          ) > 0.05
        for: 5m
        labels:
          severity: warning
          service: haystack
        annotations:
          summary: "High error rate in Haystack operations"
          description: "Error rate is {{ $value | humanizePercentage }} over the last 5 minutes"
      
      # Slow operation alert
      - alert: HaystackSlowOperations
        expr: |
          histogram_quantile(0.95, rate(haystack_operation_duration_seconds_bucket[5m])) > 5
        for: 5m
        labels:
          severity: warning
          service: haystack
        annotations:
          summary: "Haystack operations are slow"
          description: "95th percentile latency is {{ $value }}s"
      
      # Elasticsearch health alert
      - alert: ElasticsearchUnhealthy
        expr: haystack_elasticsearch_health < 2
        for: 2m
        labels:
          severity: critical
          service: haystack
        annotations:
          summary: "Elasticsearch cluster is unhealthy"
          description: "Elasticsearch status is not green"
      
      # Large tree alert
      - alert: HaystackLargeTreeWarning
        expr: histogram_quantile(0.99, haystack_tree_size_nodes_bucket) > 1000
        for: 10m
        labels:
          severity: info
          service: haystack
        annotations:
          summary: "Large trees being processed"
          description: "99th percentile tree size is {{ $value }} nodes"
      
      # Memory usage alert (from response size)
      - alert: HaystackLargeResponses
        expr: histogram_quantile(0.95, rate(haystack_response_size_bytes_bucket[5m])) > 10485760
        for: 5m
        labels:
          severity: warning
          service: haystack
        annotations:
          summary: "Large response sizes detected"
          description: "95th percentile response size is {{ $value | humanize1024 }}B"
```

### 6. Dashboard Configuration

#### Grafana Dashboard JSON

```json
{
  "dashboard": {
    "title": "Haystack Tree Navigation Monitoring",
    "panels": [
      {
        "title": "Operations Rate",
        "targets": [
          {
            "expr": "sum(rate(haystack_operations_total[5m])) by (operation)"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Error Rate",
        "targets": [
          {
            "expr": "sum(rate(haystack_errors_total[5m])) by (operation, error_type)"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Operation Latency (p50, p95, p99)",
        "targets": [
          {
            "expr": "histogram_quantile(0.5, rate(haystack_operation_duration_seconds_bucket[5m]))"
          },
          {
            "expr": "histogram_quantile(0.95, rate(haystack_operation_duration_seconds_bucket[5m]))"
          },
          {
            "expr": "histogram_quantile(0.99, rate(haystack_operation_duration_seconds_bucket[5m]))"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Tree Size Distribution",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, haystack_tree_size_nodes_bucket)"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Active Workflows",
        "targets": [
          {
            "expr": "haystack_active_workflows"
          }
        ],
        "type": "stat"
      },
      {
        "title": "Elasticsearch Health",
        "targets": [
          {
            "expr": "haystack_elasticsearch_health"
          }
        ],
        "type": "stat",
        "thresholds": {
          "mode": "absolute",
          "steps": [
            {"color": "red", "value": 0},
            {"color": "yellow", "value": 1},
            {"color": "green", "value": 2}
          ]
        }
      }
    ]
  }
}
```

### 7. Performance Profiling

#### Implementation: Async Profiler Integration

```python
# File: n8n/haystack-service/monitoring/profiling.py

import asyncio
import cProfile
import pstats
import io
from datetime import datetime
import os

class AsyncProfiler:
    """Async-aware profiler for production debugging"""
    
    def __init__(self, enabled: bool = False):
        self.enabled = enabled or os.getenv("ENABLE_PROFILING", "false").lower() == "true"
        self.profiler = None
        
    async def profile_operation(self, operation_name: str, func, *args, **kwargs):
        """Profile an async operation"""
        if not self.enabled:
            return await func(*args, **kwargs)
        
        self.profiler = cProfile.Profile()
        self.profiler.enable()
        
        try:
            result = await func(*args, **kwargs)
            return result
        finally:
            self.profiler.disable()
            
            # Save profile data
            timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
            filename = f"profiles/{operation_name}_{timestamp}.prof"
            
            os.makedirs("profiles", exist_ok=True)
            self.profiler.dump_stats(filename)
            
            # Log top functions
            s = io.StringIO()
            ps = pstats.Stats(self.profiler, stream=s).sort_stats('cumulative')
            ps.print_stats(10)
            
            logger.info(
                "profiling_completed",
                operation=operation_name,
                profile_file=filename,
                top_functions=s.getvalue()
            )

# Usage in endpoints
profiler = AsyncProfiler()

@app.get("/get_complete_tree/{workflow_id}")
async def get_complete_tree(workflow_id: str, max_depth: int = 5):
    return await profiler.profile_operation(
        "get_complete_tree",
        _get_complete_tree_impl,
        workflow_id,
        max_depth
    )
```

### 8. Deployment Configuration

#### Docker Compose Updates

```yaml
# File: n8n/docker-compose.haystack.yml (additions)

services:
  haystack-service:
    environment:
      - OTEL_EXPORTER_OTLP_ENDPOINT=tempo:4317
      - ENABLE_METRICS=true
      - ENABLE_TRACING=true
      - LOG_LEVEL=INFO
    labels:
      - "prometheus.io/scrape=true"
      - "prometheus.io/port=8000"
      - "prometheus.io/path=/metrics"

  # Monitoring stack
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./monitoring/alerts.yml:/etc/prometheus/alerts.yml
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    networks:
      - backend

  grafana:
    image: grafana/grafana:latest
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - ./monitoring/dashboards:/etc/grafana/provisioning/dashboards
      - grafana_data:/var/lib/grafana
    ports:
      - "3000:3000"
    networks:
      - backend

  tempo:
    image: grafana/tempo:latest
    command: [ "-config.file=/etc/tempo.yaml" ]
    volumes:
      - ./monitoring/tempo.yaml:/etc/tempo.yaml
      - tempo_data:/tmp/tempo
    ports:
      - "4317:4317"  # OTLP gRPC
    networks:
      - backend

  loki:
    image: grafana/loki:latest
    volumes:
      - ./monitoring/loki.yaml:/etc/loki/local-config.yaml
      - loki_data:/loki
    ports:
      - "3100:3100"
    networks:
      - backend

volumes:
  prometheus_data:
  grafana_data:
  tempo_data:
  loki_data:
```

### 9. Implementation Timeline

#### Phase 1: Core Metrics (Day 1)
- [ ] Implement MetricsCollector class
- [ ] Add endpoint decorators
- [ ] Deploy Prometheus endpoint
- [ ] Basic Grafana dashboard

#### Phase 2: Tracing & Logging (Day 2)
- [ ] OpenTelemetry integration
- [ ] Structured logging setup
- [ ] Log aggregation with Loki
- [ ] Trace visualization with Tempo

#### Phase 3: Alerting & Analysis (Day 3)
- [ ] Configure Prometheus alerts
- [ ] Set up alert routing (AlertManager)
- [ ] Performance profiling tools
- [ ] Documentation and runbooks

#### Phase 4: Testing & Optimization (Day 4)
- [ ] Load testing with monitoring
- [ ] Alert threshold tuning
- [ ] Dashboard refinement
- [ ] Team training

## Success Criteria

1. **Visibility**: All operations tracked with <1% sampling loss
2. **Performance**: Monitoring overhead <5% of request latency
3. **Alerting**: Critical issues detected within 2 minutes
4. **Debugging**: Distributed traces available for 100% of errors
5. **Compliance**: All PII excluded from logs and metrics

## Risk Mitigation

1. **Performance Impact**: Use sampling for high-volume operations
2. **Storage Growth**: Implement retention policies (30 days metrics, 7 days logs)
3. **Alert Fatigue**: Start with critical alerts only, tune thresholds weekly
4. **Security**: No sensitive data in metrics labels or log messages

## Additional Considerations

1. **Multi-region**: Consider federation for geographic distribution
2. **High Availability**: Deploy monitoring stack with redundancy
3. **Cost Management**: Monitor storage costs, implement data lifecycle
4. **Integration**: Connect to existing incident management systems

This comprehensive monitoring implementation will provide complete observability for the Haystack tree navigation system, enabling proactive issue detection, performance optimization, and rapid debugging in production environments.